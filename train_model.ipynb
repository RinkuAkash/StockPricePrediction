{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring spark context to enable aws services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (SparkConf().set(\"spark.executor.extraJavaOptions\",\n",
    "                       \"-Dcom.amazonaws.services.s3.enableV4=true\"\n",
    "                       ).set(\"spark.driver.extraJavaOptions\",\n",
    "                            \"-Dcom.amazonaws.services.s3.enableV4=true\"\n",
    "                            )\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "sc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\",\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading aws credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FileToS3/aws_credentials') as f:\n",
    "    lines = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting hadoop configuration with spark credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.access.key\", lines[0])\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", lines[1])\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", \"s3-ap-south-1.amazonaws.com\")\n",
    "hadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoopConf.set(\"com.amazonaws.services.s3a.enableV4\", \"true\")\n",
    "hadoopConf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading file from aws s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_history = sparkSession.read.csv(\"s3a://historicalstockpricesbucket/GoogleHistoricalQuotes.csv\", header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date='05/11/2020', Close/Last='1403.26', Volume='1412116', Open='1378.28', High='1416.53', Low='1377.152'),\n",
       " Row(Date='05/08/2020', Close/Last='1388.37', Volume='1388068', Open='1383.13', High='1398.76', Low='1375.48'),\n",
       " Row(Date='05/07/2020', Close/Last='1372.56', Volume='1399759', Open='1365.94', High='1377.6', Low='1355.27')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_history.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: string, Close/Last: string, Volume: string, Open: string, High: string, Low: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_history.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_history = stock_history.withColumn(\"Date\", stock_history[\"Date\"].cast(\"string\")\n",
    "                                        ).withColumn(\"Close/Last\", stock_history[\"Close/Last\"].cast(\"double\")\n",
    "                                        ).withColumn(\"Volume\", stock_history[\"Volume\"].cast(\"double\")\n",
    "                                        ).withColumn(\"Open\", stock_history[\"Open\"].cast(\"double\")\n",
    "                                        ).withColumn(\"High\", stock_history[\"High\"].cast(\"double\")\n",
    "                                        ).withColumn(\"Low\", stock_history[\"Low\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close/Last: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_history.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <td>1542</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>01/02/2015</td>\n",
       "      <td>12/31/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Close/Last</th>\n",
       "      <td>1542</td>\n",
       "      <td>892.6793313229597</td>\n",
       "      <td>265.5001569983217</td>\n",
       "      <td>492.55</td>\n",
       "      <td>1526.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>1542</td>\n",
       "      <td>1750053.9889753566</td>\n",
       "      <td>878982.4552627216</td>\n",
       "      <td>7932.0</td>\n",
       "      <td>1.11535E7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Open</th>\n",
       "      <td>1542</td>\n",
       "      <td>892.3791491569398</td>\n",
       "      <td>264.8867229772171</td>\n",
       "      <td>494.65</td>\n",
       "      <td>1525.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>1542</td>\n",
       "      <td>900.3975564202326</td>\n",
       "      <td>267.9636915346692</td>\n",
       "      <td>495.976</td>\n",
       "      <td>1532.1063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>1542</td>\n",
       "      <td>884.2809206225694</td>\n",
       "      <td>262.58149893525564</td>\n",
       "      <td>487.56</td>\n",
       "      <td>1521.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0                   1                   2           3  \\\n",
       "summary     count                mean              stddev         min   \n",
       "Date         1542                None                None  01/02/2015   \n",
       "Close/Last   1542   892.6793313229597   265.5001569983217      492.55   \n",
       "Volume       1542  1750053.9889753566   878982.4552627216      7932.0   \n",
       "Open         1542   892.3791491569398   264.8867229772171      494.65   \n",
       "High         1542   900.3975564202326   267.9636915346692     495.976   \n",
       "Low          1542   884.2809206225694  262.58149893525564      487.56   \n",
       "\n",
       "                     4  \n",
       "summary            max  \n",
       "Date        12/31/2019  \n",
       "Close/Last     1526.69  \n",
       "Volume       1.11535E7  \n",
       "Open           1525.07  \n",
       "High         1532.1063  \n",
       "Low             1521.4  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_history.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_dataframe = stock_history.select(['Open', 'High', 'Low', 'Close/Last'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+----------+\n",
      "|   Open|     High|     Low|Close/Last|\n",
      "+-------+---------+--------+----------+\n",
      "|1378.28|  1416.53|1377.152|   1403.26|\n",
      "|1383.13|  1398.76| 1375.48|   1388.37|\n",
      "|1365.94|   1377.6| 1355.27|   1372.56|\n",
      "|1361.69|1371.1199| 1347.29|    1347.3|\n",
      "|1337.92|  1373.94| 1337.46|   1351.11|\n",
      "|1308.23|  1327.66|  1299.0|    1326.8|\n",
      "| 1328.5|1352.0695|  1311.0|   1320.61|\n",
      "|1324.88|  1352.82| 1322.49|   1348.66|\n",
      "|1341.46|  1359.99| 1325.34|   1341.48|\n",
      "|1287.93|  1288.05|  1232.2|   1233.67|\n",
      "| 1296.0|  1296.15|  1269.0|   1275.88|\n",
      "|1261.17|   1280.4| 1249.45|   1279.31|\n",
      "|1271.55|  1293.31| 1265.67|   1276.31|\n",
      "|1245.54|1285.6133|  1242.0|   1263.21|\n",
      "| 1247.0|  1254.27| 1209.71|   1216.34|\n",
      "| 1271.0|   1281.6| 1261.37|   1266.61|\n",
      "|1284.85|  1294.43| 1271.23|   1283.25|\n",
      "| 1274.1|   1279.0| 1242.62|   1263.47|\n",
      "|1245.61|  1280.46|  1240.4|   1262.47|\n",
      "|1245.09|  1282.07| 1236.93|   1269.23|\n",
      "+-------+---------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "required_dataframe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorizing feature columns into features column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=['Open', 'High', 'Low'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|Close/Last|\n",
      "+--------------------+----------+\n",
      "|[1378.28,1416.53,...|   1403.26|\n",
      "|[1383.13,1398.76,...|   1388.37|\n",
      "|[1365.94,1377.6,1...|   1372.56|\n",
      "|[1361.69,1371.119...|    1347.3|\n",
      "|[1337.92,1373.94,...|   1351.11|\n",
      "|[1308.23,1327.66,...|    1326.8|\n",
      "|[1328.5,1352.0695...|   1320.61|\n",
      "|[1324.88,1352.82,...|   1348.66|\n",
      "|[1341.46,1359.99,...|   1341.48|\n",
      "|[1287.93,1288.05,...|   1233.67|\n",
      "|[1296.0,1296.15,1...|   1275.88|\n",
      "|[1261.17,1280.4,1...|   1279.31|\n",
      "|[1271.55,1293.31,...|   1276.31|\n",
      "|[1245.54,1285.613...|   1263.21|\n",
      "|[1247.0,1254.27,1...|   1216.34|\n",
      "|[1271.0,1281.6,12...|   1266.61|\n",
      "|[1284.85,1294.43,...|   1283.25|\n",
      "|[1274.1,1279.0,12...|   1263.47|\n",
      "|[1245.61,1280.46,...|   1262.47|\n",
      "|[1245.09,1282.07,...|   1269.23|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectored_dataframe = vectorAssembler.transform(required_dataframe)\n",
    "vectored_dataframe = vectored_dataframe.select(['features', 'Close/Last'])\n",
    "vectored_dataframe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression(featuresCol = 'features', labelCol='Close/Last',\n",
    "                             maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = regressor.fit(vectored_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = vectored_dataframe.randomSplit([0.7, 0.3])\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+--------------------+\n",
      "|        prediction|Close/Last|            features|\n",
      "+------------------+----------+--------------------+\n",
      "| 498.5295543328183|    500.87|[494.65,503.23,49...|\n",
      "| 498.0918183915951|    496.18|[498.84,502.98,49...|\n",
      "| 500.2719133588521|    496.17|[504.76,504.92,49...|\n",
      "| 512.3496653709844|    518.73|[510.75,519.9,504.2]|\n",
      "|501.14509816805764|    495.39|[511.56,513.05,48...|\n",
      "| 523.2456857063205|     527.7|[516.9,529.46,516...|\n",
      "|510.99735692010915|    517.15|[517.18,518.6,502.8]|\n",
      "| 524.0574061498271|    528.86|[519.7,529.78,517...|\n",
      "| 528.2968952484052|    534.39|[521.48,536.33,51...|\n",
      "| 520.4976672967623|    516.18|[522.51,524.7,515...|\n",
      "| 518.6106905403567|     513.8|[522.74,523.1,513...|\n",
      "| 527.9918045241925|     530.7|[523.99,533.46,52...|\n",
      "| 522.1005799184527|    519.98|[525.7,525.87,517...|\n",
      "| 530.8855585973517|    530.59|[527.0,534.56,526...|\n",
      "| 527.8777795272779|    526.98|[527.13,531.0,523...|\n",
      "|  519.911661200769|    511.17|[527.25,530.98,50...|\n",
      "| 525.7794101869372|    526.66|[527.6,528.0,522.52]|\n",
      "| 531.0015036739208|    528.34|[527.8,533.92,527.1]|\n",
      "| 526.6946995216687|     527.2| [528.0,528.3,524.0]|\n",
      "| 529.6910607370038|    530.42|[528.09,531.15,52...|\n",
      "+------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"prediction\",'Close/Last','features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluator = RegressionEvaluator(predictionCol='prediction', labelCol='Close/Last', metricName='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared on test data = 0.999227\n"
     ]
    }
   ],
   "source": [
    "print('R Squared on test data = %g' % model_evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
